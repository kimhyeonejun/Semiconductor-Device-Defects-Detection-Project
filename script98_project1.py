# -*- coding: utf-8 -*-
"""script98_project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hiuds0_sLzGrAc3l4ZDv9pXb5_ogX-AI
"""

import numpy as np
import scipy as sp
import scipy.stats
import pandas as pd
import matplotlib.pyplot as plt
import random

data = pd.read_csv('data98_semi_train.csv')  #훈련 데이터 셋

data

"""#이상적인 모델 가정"""

#Q1
sum_defect = sum(data['Label'] == 'defect')
sum_normal = sum(data['Label'] == 'normal')
#정상적인 칩을 생산하여 얻을 수 있는 이익은 900원 이고 불량인 칩을 생산하면 보게 되는 손해는 1100원이다. 따라서 훈련 데이터 셋을 통해 계산할 수 있는 총 이득은 다음과 같다.
cost = 900 * sum_normal - 1100 * sum_defect

print(sum_defect, sum_normal)

print(cost)

#다음으로 데이터 분석을 통하여 불량을 완벽히 판별하는 이상적인 모델을 만들었을 경우 총 이득을 구해보자. 이 경우 정상적인 칩을 정상적으로 판별하였을 때 900원의 이득을 얻을 수 있고 불량인 칩을 판별한 경우 100원의 손해만 본다. 따라서 훈련 데이텃 셋을 통해
#계산할 수 있는 총 이득은 다음과 같다.
cost = 900 * sum_normal - 100 * sum_defect

print(cost)

#마지막으로 Precision이 90%이고 Recall이 90%일 때 총 이득을 계산한다. Precision이 90%이므로 정상 칩의 90%를 정확하게 예측한다고 할 수 있다. 따라서 이를 통해 TP와 TN을 구할 수 있다.
TP = sum_defect * 0.9
FN = sum_defect * 0.1
#다음으로 Recall이 90%가 되어야 하므로 Precision을 통해 얻은 TP를 바탕으로 Recall을 계산 해 본다. Recall은 TP/(TP+FP)로 계산할 수 있으므로 FP = TP / 9 이다.
FP = TP / 9
#TN은 전체 갯수에서 TP, FN, FP를 뺸 값과 동일하다.
TN = sum_defect + sum_normal - TP - FN - FP

#각각을 출력하면 다음과 같다.
print(TP, FN, FP, TN)
#즉, TP = 666, FN = 74, FP = 74, TN = 10677이므로 이를 통해 총 이득을 계산할 수 있다.
profit = 900 * TN - 100 * TP - 100 * FP - 1100 * FN

print(profit)

"""#EDA 및 데이터 전처리"""

#Q2
#먼저, 데이터를 요약한다.
data.info()

data.describe()

data.boxplot()

data_var = data.iloc[:,1:]
data_matrix = pd.DataFrame({"Var": data_var.columns})
data_matrix ['mean'] = data_var.mean()
data_matrix ['variance'] = data_var.var()

plt.plot(data_var.mean())

plt.plot(data_var.var())

#이제 결측치를 처리할 것이다.
#각 feature에 대하여 결측치를 조사하면 다음과 같다.
data.isna().sum(axis=0)

max(data.isna().sum(axis=0))

max(data.isna().sum(axis=1))

data.isna().sum(axis=1)

print('# of variables with missing:', (data.isna().sum(axis=0)>0).sum() )
print('# of samples with missing:', (data.isna().sum(axis=1)>0).sum() )
print('# of total missing:',data.isna().sum().sum() )

plt.plot(data.isna().sum(axis=0))

plt.plot(data.isna().sum(axis=1))

data_clean = data.dropna(axis = 1, thresh = 11491 - 300)
print( data_clean.shape )
print('# of total missing:',data_clean.isna().sum().sum() )

plt.plot(data_clean.isna().sum(axis = 0))

plt.plot(data_clean.isna().sum(axis = 1))

data_clean = data_clean.dropna(axis = 0, thresh = 531 - 7)
print( data_clean.shape )
print('# of total missing:',data_clean.isna().sum().sum() )

plt.plot(data_clean.isna().sum(axis=0))

plt.plot(data_clean.isna().sum(axis=1))

data_clean.shape

#결측치 대치
data_clean = data_clean.fillna( data_clean.mean() )
data = data_clean
print( data.shape )
print('# of total missing:',data.isna().sum().sum() )

data

#다음으로 이상치를 탐색한다.
#Label을 저장한다. 저장하지 않으면 오류가 발생
data_label = data['Label']
sp.stats.zscore(data.iloc[:,1:])

# 3sigma (99.73%) 바깥 쪽의 것을 이상치로 한다
outlier_idx = np.abs( sp.stats.zscore(data.iloc[:,1:]) ) > 3
print('# of outliers:', outlier_idx.sum().sum() )

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(data.iloc[:,1:])
data_scaled = scaler.transform(data.iloc[:,1:])

data = pd.DataFrame(data_scaled, columns = data.columns[1:])

data

data.insert(0, 'Label', data_label)

data

#p-value 구하기
#data_label와 data_feature를 이용한다.
Y = data_clean.iloc[:,1:]
out = pd.DataFrame({"Var": Y.columns})
slist = np.zeros(out.shape[0])
plist = np.zeros(out.shape[0])
for i in range(out.shape[0]):
    x = data_clean["Label"]
    y = Y.iloc[:,i]
    n = x.unique()
    ylist = []
    for j in range(len(n)):
        ylist.append(y[x==n[j]])
    r = sp.stats.ttest_ind(*ylist)
    s = r.statistic
    p = r.pvalue
    slist[i] = s
    plist[i] = p
out['stat'] = slist
out['pvalue'] = plist

out.sort_values(by = 'pvalue', ascending = False)

out['pvalue'].isna().sum()

data_pvalue = data.iloc[:,1:]
num = 0
for i in data_pvalue:
  if (out['pvalue'].isna())[num] == 1:
    data_pvalue = data_pvalue.drop([i], axis = 1)
    data = data.drop([i], axis = 1)
  num = num + 1

data

out = out.dropna()

out

out.sort_values(by = 'pvalue', ascending = False)

print(out.shape[0])

#p-value가 큰 경우 지워준다.
num = 0
out = out.reindex(range(0, out.shape[0]))
for i in data_pvalue:
    if out['pvalue'][num] >= 0.05:
      data_pvalue = data_pvalue.drop([i], axis = 1)
      data = data.drop([i], axis = 1)
    num = num + 1

data

#최종적으로 전처리된
print(data.shape)

"""#예측 모델 만들기"""

#전처리한 데이터를 train_set으로 변형
xtrain = data.iloc[:,1:]
ytrain = pd.DataFrame({'Label': data['Label']})

#train_test_split 함수가 자동적으로 해주는 작업을 직접 해주어야 함
#데이터 type을 혹시 모르니 모두 float로 변환
xtrain = xtrain.astype(float)
#Label도 마찬가지로 training을 원할하게 하기 위해 1차원 array 형식으로 퍼준다
ytrain = ytrain.values.ravel()

#추가로 f1 score를 구할 때 편리하기 위해 label의 class를 0,1로 바꾼다
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
ytrain = label_encoder.fit_transform(ytrain)

"""#Logistic, Logistic(PCA), KNN, KNN(PCA), QDA, QDA(PCA), SVM, SVM(PCA), RandomForest, RandomForest(PCA), Neural_Net, Neural_Net(PCA) 순서대로 진행

#Logistic Regression
"""

#Logistic regression 튜닝, cross validation는 3으로 하여 훈련
#C는 Ridge 규제화의 alpha의 역수 값
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
params = {'C': (0.001, 0.01, 0.1, 1.0, 10.0, 100.0)}
f = GridSearchCV( LogisticRegression(max_iter = 10000, penalty = 'l2', solver = 'lbfgs', class_weight= 'balanced' ), params, cv = 3 )
f.fit(xtrain, ytrain)
f.best_estimator_

"""#Logistic Regression (PCA)"""

#PCA해보기
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(xtrain)

#전체 데이터에서 설명이 되는 variance를 그래프로 나타내기
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

#n_component 변수를 지정하여 위 그래프에서 0.9이상이 될 때 components의 수를 측정
n_component = 0
for i in range(xtrain.shape[1]):
  if np.cumsum(pca.explained_variance_ratio_)[i] > 0.9:
    print('number of principal components :', i)
    n_component = i
    break

#pca로 train 후 ztrain에 저장
from sklearn.decomposition import PCA
pca = PCA(n_components = n_component)
pca.fit(xtrain)
ztrain = pd.DataFrame( pca.transform(xtrain) )

#Logistic regression 튜닝, cross validation는 3으로 하여 훈련
#C는 Ridge 규제화의 alpha의 역수 값
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
params = {'C': (0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0)}
f = GridSearchCV( LogisticRegression(max_iter = 10000, penalty = 'l2', solver = 'lbfgs', class_weight= 'balanced' ), params,  error_score='raise', cv = 3 )
f.fit(ztrain, ytrain)
f.best_estimator_

"""#KNN"""

#KNN 모델을 만들 것이다. n_neighbors를 튜닝해 줄 것이고 weights는 distance로 할 것이다.
from sklearn.neighbors import KNeighborsClassifier
params = {'n_neighbors': np.arange(20,100)}
from sklearn.model_selection import GridSearchCV
f = GridSearchCV(KNeighborsClassifier(weights = 'distance'),params, cv = 3)
f.fit(xtrain,ytrain)
f.best_params_

f.best_estimator_

"""#KNN (PCA)"""

#PCA해보기
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(xtrain)

#전체 데이터에서 설명이 되는 variance를 그래프로 나타내기
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

#n_component 변수를 지정하여 위 그래프에서 0.9이상이 될 때 components의 수를 측정
n_component = 0
for i in range(xtrain.shape[1]):
  if np.cumsum(pca.explained_variance_ratio_)[i] > 0.9:
    print('number of principal components :', i)
    n_component = i
    break

#pca로 train 후 ztrain에 저장
from sklearn.decomposition import PCA
pca = PCA(n_components = n_component)
pca.fit(xtrain)
ztrain = pd.DataFrame( pca.transform(xtrain) )

#KNN 모델을 만들 것이다. n_neighbors를 튜닝해 줄 것이고 weights는 distance로 할 것이다.
from sklearn.neighbors import KNeighborsClassifier
params = {'n_neighbors': np.arange(20,100)}
from sklearn.model_selection import GridSearchCV
f = GridSearchCV(KNeighborsClassifier(weights = 'distance'),params)
f.fit(ztrain,ytrain)
f.best_params_

"""#Bernoulli NB


"""

#GridSearchCV로 a값 지정하고 LDA, QDA는 교차검증
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import GridSearchCV
params = {'alpha': 10 ** np.linspace(-3,3,21)}
f = GridSearchCV(BernoulliNB(), params, cv = 3)
f.fit(xtrain,ytrain)

f.best_params_

"""#Bernoulli NB (PCA)"""

#PCA해보기
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(xtrain)

#전체 데이터에서 설명이 되는 variance를 그래프로 나타내기
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

#n_component 변수를 지정하여 위 그래프에서 0.9이상이 될 때 components의 수를 측정
n_component = 0
for i in range(xtrain.shape[1]):
  if np.cumsum(pca.explained_variance_ratio_)[i] > 0.9:
    print('number of principal components :', i)
    n_component = i
    break

#pca로 train 후 ztrain에 저장
from sklearn.decomposition import PCA
pca = PCA(n_components = n_component)
pca.fit(xtrain)
ztrain = pd.DataFrame( pca.transform(xtrain) )

#GridSearchCV로 a값 지정하고 LDA, QDA는 교차검증
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import GridSearchCV
params = {'alpha': 10 ** np.linspace(-3,3,21)}
f = GridSearchCV(BernoulliNB(), params)
f.fit(ztrain,ytrain)

"""#LDA"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
f = LinearDiscriminantAnalysis(store_covariance=True, solver = 'svd')
f.fit(xtrain,ytrain)

"""#LDA (PCA)"""

#PCA해보기
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(xtrain)

#전체 데이터에서 설명이 되는 variance를 그래프로 나타내기
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

#n_component 변수를 지정하여 위 그래프에서 0.9이상이 될 때 components의 수를 측정
n_component = 0
for i in range(xtrain.shape[1]):
  if np.cumsum(pca.explained_variance_ratio_)[i] > 0.9:
    print('number of principal components :', i)
    n_component = i
    break

#pca로 train 후 ztrain에 저장
from sklearn.decomposition import PCA
pca = PCA(n_components = n_component)
pca.fit(xtrain)
ztrain = pd.DataFrame( pca.transform(xtrain) )

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
f = LinearDiscriminantAnalysis(store_covariance=True)
f.fit(ztrain,ytrain)

"""#QDA"""

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
f = QuadraticDiscriminantAnalysis(store_covariance=True)
f.fit(xtrain,ytrain)

"""#QDA (PCA)"""

#PCA해보기
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(xtrain)

#전체 데이터에서 설명이 되는 variance를 그래프로 나타내기
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

#n_component 변수를 지정하여 위 그래프에서 0.9이상이 될 때 components의 수를 측정
n_component = 0
for i in range(xtrain.shape[1]):
  if np.cumsum(pca.explained_variance_ratio_)[i] > 0.9:
    print('number of principal components :', i)
    n_component = i
    break

#pca로 train 후 ztrain에 저장
from sklearn.decomposition import PCA
pca = PCA(n_components = n_component)
pca.fit(xtrain)
ztrain = pd.DataFrame( pca.transform(xtrain) )

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
f = QuadraticDiscriminantAnalysis(store_covariance=True)
f.fit(ztrain,ytrain)

"""#SVM"""



"""# SVM (PCA)"""

#PCA해보기
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(xtrain)

#전체 데이터에서 설명이 되는 variance를 그래프로 나타내기
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

#n_component 변수를 지정하여 위 그래프에서 0.9이상이 될 때 components의 수를 측정
n_component = 0
for i in range(xtrain.shape[1]):
  if np.cumsum(pca.explained_variance_ratio_)[i] > 0.9:
    print('number of principal components :', i)
    n_component = i
    break

#pca로 train 후 ztrain에 저장
from sklearn.decomposition import PCA
pca = PCA(n_components = n_component)
pca.fit(xtrain)
ztrain = pd.DataFrame( pca.transform(xtrain) )

"""#RandomForest"""



"""# RandomForest (PCA)"""

#PCA해보기
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(xtrain)

#전체 데이터에서 설명이 되는 variance를 그래프로 나타내기
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

#n_component 변수를 지정하여 위 그래프에서 0.9이상이 될 때 components의 수를 측정
n_component = 0
for i in range(xtrain.shape[1]):
  if np.cumsum(pca.explained_variance_ratio_)[i] > 0.9:
    print('number of principal components :', i)
    n_component = i
    break

#pca로 train 후 ztrain에 저장
from sklearn.decomposition import PCA
pca = PCA(n_components = n_component)
pca.fit(xtrain)
ztrain = pd.DataFrame( pca.transform(xtrain) )

"""#ANN"""



"""# ANN (PCA)"""

#PCA해보기
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(xtrain)

#전체 데이터에서 설명이 되는 variance를 그래프로 나타내기
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

#n_component 변수를 지정하여 위 그래프에서 0.9이상이 될 때 components의 수를 측정
n_component = 0
for i in range(xtrain.shape[1]):
  if np.cumsum(pca.explained_variance_ratio_)[i] > 0.9:
    print('number of principal components :', i)
    n_component = i
    break

#pca로 train 후 ztrain에 저장
from sklearn.decomposition import PCA
pca = PCA(n_components = n_component)
pca.fit(xtrain)
ztrain = pd.DataFrame( pca.transform(xtrain) )

"""#모델 테스트"""

#f1값 구하기
from sklearn.metrics import f1_score
probabilities = f.predict_proba(xtrain)[:,1]
threshold = [0.1,0.2,0.3,0.4,0.5,0.6,0.999]
f1_list = []
for i in range(len(threshold)):
  predictions = (probabilities > threshold[i]).astype(int)
  f1 = f1_score(ytrain, predictions, average = 'weighted')
  f1_list.append(f1)
print(f1_list)
print(max(f1_list))

plt.plot(probabilities)

"""#모델 파라메터 (PCA)"""

#f1값 구하기
from sklearn.metrics import f1_score
probabilities = f.predict_proba(ztrain)[:,1]
threshold = [0.1,0.2,0.3,0.4,0.7,0.9,0.999]
f1_list = []
for i in range(len(threshold)):
  predictions = (probabilities > threshold[i]).astype(int)
  f1 = f1_score(ytrain, predictions, average = 'weighted')
  f1_list.append(f1)
print(f1_list)
print(max(f1_list))

plt.plot(probabilities)

"""#파일 출력"""

#out = f.predict(xtest)

# 파일 이름 정의 (학번에 해당하는 부분을 적절히 변경)
#file_name = "project01_2021171029.txt"

# 파일에 예측값 작성
#with open(file_name, 'w') as file:
#    for prediction in out:
#        file.write(str(prediction) + '\n')